{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/FlorianWolff95/DHBW_BI_S24/blob/main/ML_Fallstudie_Online_Barber_Shop.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFzfSZZpys_D"
      },
      "source": [
        "# Case Study \"Online Barber Shop\"\n",
        "Sie finden unter `./Daten/Barber/` den Ausschnitt eines Datensatzes einer **Direct-to-Consumer (D2C) Brand**, die über einen Onlineshop **Rasierer** und **Rasierzubehör** verkauft. Für das Unternehmen ist es enorm wichtig, dass es die voraussichtlichen **Absätze der nächsten 21 Tage** kennt, da es Waren im Voraus bei seinen Lieferanten bestellen muss. Je genauer die Absatzprognose, desto weniger Stockouts bzw. unnötige Bestände hat das Unternehmen und desto besser die Marge.\n",
        "\n",
        "Bisher ist es dem Unternehmen nicht gelungen, gute Prognosen zu erstellen. Um dies zu ändern, werden Sie nun beauftragt, eine Prognose mithilfe von ML-Algorithmen anzufertigen. Dazu stellt Ihnen das Unternehmen einen ersten Datensatz aus seinem Shopsystem zur Verfügung. In diesem sind **3 Produkte** enthalten (Alaunstein, Bartbalm und ein Rasierer) und Sie sollen für diese jeweils eine Absatzprogonse erstellen. Bitte orientieren Sie sich bei der Bearbeitung der Fallstudie an der in diesem Notebook vorgeschlagenen Struktur und greifen Sie auf die Notebooks zurück, die wir bereits in der Vorlesung gemeinsam bearbeitet haben.\n",
        "\n",
        "Die Struktur Ihres Notebooks sollte dabei wie folgt aussehen und u.a. die folgenden Fragen beantworten:\n",
        "\n",
        "1. **Daten laden & aufbereiten**. Wichtig: Nachdem Sie die Daten geladen haben, müssen Sie diese zunächst auf Tage und je Produkt aggregieren.\n",
        "2. **Datenexploration**, d.h. verschaffen Sie sich einen Überblick zum Datensatz. Sie sollten folgende Fragen beantworten:\n",
        "  - Wie viele Absätze wurden über welchen Zeitraum je Produkt dokumentiert?\n",
        "  - Was ist der Gesamtumsatz je Produkt?\n",
        "  - Wie sind die Produktverkäufe im Zeitverlauf? Visualisieren Sie beispielweise die monatlichen/wöchentlichen Verkaufsmengen. Dies ist wichtig, um Trends oder Saisonalität in Zeitreihen zu erkennen.\n",
        "  - Wie sind die durchschnittlichen Absätze pro Wochentag?\n",
        "3. **Feature Engineering**\n",
        "  - Entwickeln Sie basirend auf Ihrer Datenexploration nützliche Features für ein Vorhersagemodell.\n",
        "  - Stellen Sie sich die Frage, mit welchen Features Sie Saisonalitäten abbilden können. Gibt es Monate in den die Absätze deutlich höher sind? Falls ja, sollten Sie ein Features einführen, das den Monat (Jan, Feb, ..., Dez) abbildet!\n",
        "  - Bilden Sie autoregressive Features, z.B. könnte es Sinn ergeben zur Progonse des Absatzes an einem Tag (z.B. 15.03.) den Absatz des Vormonats zum gleichen Datum (15.02.) heranzuziehen.\n",
        "  - Können Sie aus den zur Verfügung stehenden Daten den Stückpreis als Feature ableiten?\n",
        "4. **Prognosemodell trainieren und bewerten**\n",
        "  - Trainieren Sie ein globales Prognosemodell auf allen Daten und bewerten Sie dieses.\n",
        "  - Trainieren Sie drei lokale Prognosemodelle (für Alaunstein, Bartbalm & Rasierer) und überprüfen Sie, ob die Prognosegüte verbessert wird.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P2OU4xVnysDP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! git clone https://github.com/FlorianWolff95/DHBW_BI_S24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WORKING_DIR = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join(WORKING_DIR, \"Daten\")):\n",
        "    DATA_DIR = os.path.join(WORKING_DIR, \"DHBW_BI_S24\", \"Daten\")\n",
        "else:\n",
        "    DATA_DIR = os.path.join(WORKING_DIR, \"Daten\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\n",
        "    os.path.join(DATA_DIR, \"Barber\", \"barber_shopify_data.csv\"),\n",
        "    sep=\";\",\n",
        "    parse_dates=[\"Datum\"],\n",
        "    decimal=\",\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns = df.columns.str.replace(\"Versand Land\", \"Versand_Land\")\n",
        "\n",
        "for col in df.columns:\n",
        "    df.rename(columns={col: col.lower()}, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(df):\n",
        "\n",
        "    min_date = min(pd.to_datetime(df[\"datum\"]))\n",
        "    max_date = max(pd.to_datetime(df[\"datum\"]))\n",
        "\n",
        "    dates = (\n",
        "        pd.date_range(start=min_date, end=max_date, name=\"datum\")\n",
        "        .to_frame()\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    dates = (\n",
        "        pd.merge(dates, df[\"produkt\"], how=\"cross\")\n",
        "        .drop_duplicates()\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return pd.merge(dates, df, how=\"left\", on=[\"datum\", \"produkt\"]).fillna(0)\n",
        "\n",
        "\n",
        "df_agg = (\n",
        "    df.copy()\n",
        "    .groupby([\"datum\", \"produkt\"])\n",
        "    .agg({\"nettomenge\": \"sum\", \"bruttoumsatz\": \"sum\"})\n",
        "    .reset_index()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_agg_for_col(\n",
        "    df: pd.DataFrame,\n",
        "    group_col: list[str] | str,\n",
        "    agg_col: str,\n",
        "    agg_func: str = \"sum\",\n",
        "    sorted: bool = False,\n",
        "    ascending: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    if isinstance(group_col, str):\n",
        "        group_col = [group_col]\n",
        "    if agg_func == \"sum\":\n",
        "        agg_result = df.groupby(group_col)[agg_col].sum()\n",
        "    elif agg_func == \"mean\":\n",
        "        agg_result = df.groupby(group_col)[agg_col].mean()\n",
        "    elif agg_func == \"count\":\n",
        "        agg_result = df.groupby(group_col)[agg_col].count()\n",
        "    elif agg_func == \"min\":\n",
        "        agg_result = df.groupby(group_col)[agg_col].min()\n",
        "    elif agg_func == \"max\":\n",
        "        agg_result = df.groupby(group_col)[agg_col].max()\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid aggregation function: {agg_func}\")\n",
        "\n",
        "    agg_result = agg_result.reset_index()\n",
        "\n",
        "    if sorted:\n",
        "        agg_result = agg_result.sort_values(by=agg_col, ascending=ascending)\n",
        "\n",
        "    return agg_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agg_cols = [\"nettomenge\", \"bruttoumsatz\"]\n",
        "for col in agg_cols:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(\n",
        "        data=get_agg_for_col(df, [\"datum\", \"produkt\"], col),\n",
        "        x=\"datum\",\n",
        "        y=col,\n",
        "        hue=\"produkt\",\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in [\"nettomenge\", \"bruttoumsatz\"]:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title(f\"Summe {col} pro Produkt\")\n",
        "    sns.barplot(\n",
        "        data=get_agg_for_col(df, \"produkt\", \"nettomenge\", sorted=True),\n",
        "        x=\"produkt\",\n",
        "        y=\"nettomenge\",\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_datetime_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"year\"] = df[\"datum\"].dt.year\n",
        "    df[\"month\"] = df[\"datum\"].dt.month\n",
        "    df[\"day\"] = df[\"datum\"].dt.day\n",
        "    df[\"week\"] = df[\"datum\"].dt.isocalendar().week\n",
        "    df[\"weekday\"] = df[\"datum\"].dt.weekday\n",
        "    df[\"day_of_year\"] = df[\"datum\"].dt.dayofyear\n",
        "    df[\"quarter\"] = df[\"datum\"].dt.quarter\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_agg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_agg = add_datetime_features(df_agg)\n",
        "df_agg[\"day_name\"] = df_agg[\"datum\"].dt.day_name()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    data=get_agg_for_col(\n",
        "        df_agg, [\"day_name\", \"produkt\"], \"nettomenge\", agg_func=\"mean\"\n",
        "    ),\n",
        "    x=\"day_name\",\n",
        "    y=\"nettomenge\",\n",
        "    hue=\"produkt\",\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_agg.sort_values(by=\"datum\", inplace=True, ascending=True)\n",
        "\n",
        "df_agg[\"sales_lag1d\"] = df_agg.groupby(\"produkt\")[\"nettomenge\"].transform(\n",
        "    lambda x: x.shift(21)\n",
        ")\n",
        "df_agg[\"3d_rolling_demand\"] = df_agg.groupby(\"produkt\")[\"nettomenge\"].transform(\n",
        "    lambda x: x.shift(21).rolling(window=3).mean()\n",
        ")\n",
        "df_agg.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rolling_transform(\n",
        "    df: pd.DataFrame,\n",
        "    groupby_cols: list[str],\n",
        "    column: str,\n",
        "    shift_val: int,\n",
        "    window_size: int,\n",
        ") -> pd.DataFrame:\n",
        "    df[column + f\"_lag{shift_val}d\"] = df.groupby(groupby_cols)[column].transform(\n",
        "        lambda x: x.shift(shift_val)\n",
        "    )\n",
        "    df[column + f\"_{window_size}d_rolling_demand\"] = df.groupby(groupby_cols)[\n",
        "        column\n",
        "    ].transform(lambda x: x.shift(shift_val).rolling(window=window_size).mean())\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_agg[\"item_value\"] = df_agg[\"bruttoumsatz\"] / df_agg[\"nettomenge\"]\n",
        "df_agg.dropna(inplace=True)\n",
        "df_agg.drop(columns=\"day_name\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_agg.set_index(\"datum\", inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Models to predict 'bruttoumsatz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### inital imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    mean_absolute_percentage_error,\n",
        ")\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import joblib\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! git clone https://github.com/FlorianWolff95/DHBW_BI_S24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WORKING_DIR = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join(WORKING_DIR, \"Daten\")):\n",
        "    DATA_DIR = os.path.join(WORKING_DIR, \"DHBW_BI_S24\", \"Daten\")\n",
        "else:\n",
        "    DATA_DIR = os.path.join(WORKING_DIR, \"Daten\")\n",
        "\n",
        "data = os.path.join(DATA_DIR, \"Barber\", \"barber_shopify_data.csv\")\n",
        "\n",
        "MODEL_DIR = os.path.join(WORKING_DIR, \"models\")\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.makedirs(MODEL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "df = pd.read_csv(data, sep=\";\", parse_dates=[\"Datum\"], decimal=\",\")\n",
        "\n",
        "\n",
        "df.columns = df.columns.str.replace(\"Versand Land\", \"Versand_Land\")\n",
        "for col in df.columns:\n",
        "    df.rename(columns={col: col.lower()}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### define preprocessing functions\n",
        "### and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preprocessing functions\n",
        "def add_datetime_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"year\"] = df[\"datum\"].dt.year\n",
        "    df[\"month\"] = df[\"datum\"].dt.month\n",
        "    df[\"day\"] = df[\"datum\"].dt.day\n",
        "    df[\"week\"] = df[\"datum\"].dt.isocalendar().week.astype(int)\n",
        "    df[\"weekday\"] = df[\"datum\"].dt.weekday\n",
        "    df[\"day_of_year\"] = df[\"datum\"].dt.dayofyear\n",
        "    df[\"quarter\"] = df[\"datum\"].dt.quarter\n",
        "    return df\n",
        "\n",
        "\n",
        "def calc_avg_item_value(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df[\"item_value\"] = df[\"bruttoumsatz\"] / df[\"nettomenge\"]\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_rolling_transform(\n",
        "    df: pd.DataFrame,\n",
        "    groupby_col: str,\n",
        "    column: str,\n",
        "    shift_val: int,\n",
        "    window_size: int,\n",
        ") -> pd.DataFrame:\n",
        "    df.sort_values(by=\"datum\", inplace=True, ascending=True)\n",
        "    new_df = pd.DataFrame()\n",
        "    if f\"{column}_lag{shift_val}d\" in df.columns:\n",
        "        pass\n",
        "    else:\n",
        "        new_df[f\"{column}_lag{shift_val}d\"] = df.groupby(groupby_col)[column].transform(\n",
        "            lambda x: x.shift(shift_val)\n",
        "        )\n",
        "    agg_methods = [\"mean\", \"median\", \"std\", \"min\", \"max\"]\n",
        "    for agg_method in agg_methods:\n",
        "        new_df[\n",
        "            f\"{column}_lag{shift_val}d_{window_size}d_rolling_{agg_method}\"\n",
        "        ] = df.groupby(groupby_col)[column].transform(\n",
        "            lambda x: x.shift(shift_val).rolling(window=window_size).agg(agg_method)\n",
        "        )\n",
        "    df = pd.concat([df, new_df], axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "# preprocess data\n",
        "df_agg = (\n",
        "    df.copy()\n",
        "    .groupby([\"datum\", \"produkt\"])\n",
        "    .agg({\"nettomenge\": \"sum\", \"bruttoumsatz\": \"sum\"})\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "\n",
        "# preprocess_data(df_agg)\n",
        "add_datetime_features(df_agg)\n",
        "calc_avg_item_value(df_agg)\n",
        "\n",
        "rolling_kwargs = {}\n",
        "for col in [\"nettomenge\", \"item_value\"]:\n",
        "    for shift_val in [30, 60, 90, 180, 365]:\n",
        "        for window_size in [3, 7, 14, 30, 60, 90, 180, 365]:\n",
        "            rolling_kwargs[f\"{col}_{shift_val}_{window_size}\"] = {\n",
        "                \"groupby_col\": \"produkt\",\n",
        "                \"column\": col,\n",
        "                \"shift_val\": shift_val,\n",
        "                \"window_size\": window_size,\n",
        "            }\n",
        "\n",
        "for col, kwargs in rolling_kwargs.items():\n",
        "    df_agg = create_rolling_transform(df_agg, **kwargs)\n",
        "df_agg.set_index(\"datum\", inplace=True)\n",
        "df_agg.drop(columns=\"nettomenge\", inplace=True)\n",
        "\n",
        "df_agg.head()\n",
        "# create holdout set last 30 days\n",
        "holdout_split = \"2022-06-01\"\n",
        "df_agg = df_agg[df_agg.index < \"2022-07-01\"]\n",
        "holdout_data = df_agg[df_agg.index >= holdout_split]\n",
        "df_agg_train_test = df_agg[df_agg.index < holdout_split]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# item value over time\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_agg = df_agg.reset_index()\n",
        "sns.lineplot(data=df_agg, x=\"datum\", y=\"item_value\", hue=\"produkt\")\n",
        "plt.title(\"Item Value over Time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### perform GridSearchCV with TimeseriesSplit on DataFrame; save best model as JOBLIB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 500, 1000],\n",
        "    \"learning_rate\": [0.1, 0.01, 0.001],\n",
        "    \"max_depth\": [3, 5, 7, 10, None],\n",
        "}\n",
        "\n",
        "# Perform time series cross-validation and grid search for each product\n",
        "for product in df_agg_train_test[\"produkt\"].unique():\n",
        "    df_global_train_test = (\n",
        "        df_agg_train_test[df_agg_train_test[\"produkt\"] == product]\n",
        "        .drop(columns=\"produkt\")\n",
        "        .copy()\n",
        "    )\n",
        "\n",
        "    X = df_global_train_test.drop(columns=\"bruttoumsatz\")\n",
        "    y = df_global_train_test[\"bruttoumsatz\"]\n",
        "\n",
        "    model = XGBRegressor(random_state=42, objective=\"reg:squarederror\", n_jobs=-1)\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        model, param_grid, cv=tscv, scoring=\"neg_mean_squared_error\"\n",
        "    )\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    model_name = f\"{product}_model\"\n",
        "    file_name = f\"models/{model_name}.joblib\"\n",
        "\n",
        "    joblib.dump(best_model, file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### load models and predict on holdout data and visualize performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecast_df = pd.DataFrame()\n",
        "for product in df[\"produkt\"].unique():\n",
        "    model_path = f\"models/{product}_model.joblib\"\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    holdout_data_product = (\n",
        "        holdout_data[holdout_data[\"produkt\"] == product].drop(columns=\"produkt\").copy()\n",
        "    )\n",
        "    X_holdout = holdout_data_product.drop(columns=[\"bruttoumsatz\"])\n",
        "    y_holdout = holdout_data_product[\"bruttoumsatz\"].reset_index()\n",
        "\n",
        "    y_forecast_new = model.predict(X_holdout)\n",
        "    y_forecast_new = pd.DataFrame(\n",
        "        y_forecast_new, index=y_holdout[\"datum\"], columns=[\"forecast\"]\n",
        "    ).reset_index()\n",
        "    y_forecast_new[\"produkt\"] = product\n",
        "    forecast_df_temp = pd.merge(y_holdout, y_forecast_new, how=\"inner\", on=\"datum\")\n",
        "    forecast_df = pd.concat([forecast_df, forecast_df_temp], axis=0)\n",
        "\n",
        "    mse_prd = mean_squared_error(y_holdout[\"bruttoumsatz\"], y_forecast_new[\"forecast\"])\n",
        "    mae_prd = mean_absolute_error(y_holdout[\"bruttoumsatz\"], y_forecast_new[\"forecast\"])\n",
        "    mape_prd = mean_absolute_percentage_error(\n",
        "        y_holdout[\"bruttoumsatz\"], y_forecast_new[\"forecast\"]\n",
        "    )\n",
        "\n",
        "    print(f\"Errors for {product}:\")\n",
        "    print(f\"Mean Squared Error: {mse_prd}\")\n",
        "    print(f\"Mean Absolute Error: {mae_prd}\")\n",
        "    print(f\"Mean Absolute Percentage Error: {mape_prd}\\n\")\n",
        "    print(f\"{'_'*20}\\n\")\n",
        "\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    plt.title(f\"Forecast for {product}\")\n",
        "    sns.lineplot(data=forecast_df_temp, x=\"datum\", y=\"bruttoumsatz\")\n",
        "    sns.lineplot(data=forecast_df_temp, x=\"datum\", y=\"forecast\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "forecast_df_total = (\n",
        "    forecast_df.groupby([\"datum\"])\n",
        "    .agg({\"bruttoumsatz\": \"sum\", \"forecast\": \"sum\"})\n",
        "    .reset_index()\n",
        ")\n",
        "mse = mean_squared_error(\n",
        "    forecast_df_total[\"bruttoumsatz\"], forecast_df_total[\"forecast\"]\n",
        ")\n",
        "mae = mean_absolute_error(\n",
        "    forecast_df_total[\"bruttoumsatz\"], forecast_df_total[\"forecast\"]\n",
        ")\n",
        "mape = mean_absolute_percentage_error(\n",
        "    forecast_df_total[\"bruttoumsatz\"], forecast_df_total[\"forecast\"]\n",
        ")\n",
        "\n",
        "print(f\"{'_'*20}\\n\")\n",
        "print(\"Errors for the total forecast:\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Mean Absolute Percentage Error: {mape}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the forecast\n",
        "forecast_df_total = (\n",
        "    forecast_df.groupby([\"datum\"])\n",
        "    .agg({\"bruttoumsatz\": \"sum\", \"forecast\": \"sum\"})\n",
        "    .reset_index()\n",
        ")\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.lineplot(data=forecast_df_total, x=\"datum\", y=\"bruttoumsatz\", label=\"actual\")\n",
        "sns.lineplot(data=forecast_df_total, x=\"datum\", y=\"forecast\", label=\"forecast\")\n",
        "plt.title(\"Forecast vs Actual\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPFLAtpqmpfEXQ+JGHLQgmm",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
